{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import keys\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# XE api\n",
    "from xecd_rates_client import XecdClient\n",
    "\n",
    "# Google News api\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Google Cloud client library api\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_DF:\n",
    "    def __init__(self, cur_original, cur_new, news_api_key, json_google_cloud, xecd_client, rapid_api_host, rapid_api_key):\n",
    "        self.today = date.today().strftime(\"%Y/%m/%d\").replace(\"/\",\"-\")\n",
    "        self.original = cur_original\n",
    "        self.new = cur_new\n",
    "        self.newsapi = NewsApiClient(api_key=news_api_key)\n",
    "        self.client = language.LanguageServiceClient.from_service_account_json(json_google_cloud)\n",
    "        self.xecd = xecd_client\n",
    "        self.rapid_api_host = rapid_api_host\n",
    "        self.rapid_api_key = rapid_api_key\n",
    "        self.approved_classifications = [\n",
    "            'Finance',\n",
    "            'Business',\n",
    "            'Politics',\n",
    "            'Jobs & Education',\n",
    "            'Business News',\n",
    "            'Business & Industrial',\n",
    "        ]\n",
    "        \n",
    "        \n",
    "    # classify text into categories   \n",
    "    def __classify(self, text):\n",
    "        client = self.client\n",
    "        document = language.types.Document(content=text, type=language.enums.Document.Type.PLAIN_TEXT)\n",
    "        response = client.classify_text(document)\n",
    "        categories = response.categories\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for category in categories:\n",
    "            categories = category.name.split(\"/\")\n",
    "            categories.pop(0)\n",
    "            for i in range(len(categories)):\n",
    "                result.append(categories[i])\n",
    "        return result\n",
    "    \n",
    "    def __getRate(self, date, fromCur, toCur):\n",
    "        xecd = self.xecd\n",
    "        rateResult = xecd.historic_rate(date, \"12:00\", fromCur, toCur, 1)\n",
    "        rateObj={}\n",
    "        rateObj['date'] = rateResult['timestamp'][:10]\n",
    "        rateObj['rate'] = rateResult['to'][0]['mid']\n",
    "        return rateObj['rate']\n",
    "    \n",
    "    \n",
    "    # retrieve articles from google news api\n",
    "    def __retrieve_articles(self):\n",
    "        formatted_articles = []\n",
    "        pageNum = 1\n",
    "        \n",
    "        print(\"===============Start Scraping===============\")\n",
    "        \n",
    "        curr_dict = {\n",
    "            'CAD' : 'Canada Canadian',\n",
    "            'USD' : 'United States American',\n",
    "            'GBP' : 'Great Britain Pound',\n",
    "            'AUD' : 'Australia Australia',\n",
    "            'JPY' : 'Japan Japanese Yen',\n",
    "            'EUR' : 'Europe Euro European',\n",
    "            'CNY' : 'China Chinese Yen',\n",
    "            'INR' : 'India Indian Rupee'\n",
    "        }\n",
    "        \n",
    "        while True:\n",
    "            scraped = 0;\n",
    "            url = \"https://contextualwebsearch-websearch-v1.p.rapidapi.com/api/Search/NewsSearchAPI\"\n",
    "            querystring = {\"fromPublishedDate\":\"2019-06-14 00:00:00\",\"toPublishedDate\":\"2019-09-13 23:59:59\",\"autoCorrect\":\"false\",\"pageNumber\":str(pageNum),\"pageSize\":\"50\",\"q\":curr_dict[self.original],\"safeSearch\":\"false\"}\n",
    "            headers = {\n",
    "                'x-rapidapi-host': self.rapid_api_host,\n",
    "                'x-rapidapi-key': self.rapid_api_key\n",
    "            }\n",
    "\n",
    "            all_articles = requests.request(\"GET\", url, headers=headers, params=querystring).json()\n",
    "            iterationsLeft = (all_articles['totalCount'] - pageNum*50) // 50\n",
    "            articles = all_articles['value']\n",
    "\n",
    "            # creating new article object\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    info = str(article['title']) + \" \" + str(article['description']) + \" \" + str(article['body'])\n",
    "                    if len(info.split(\" \")) > 21:\n",
    "                        classifications = self.__classify(info)\n",
    "                        if len(set(classifications).intersection(set(self.approved_classifications))) != 0:\n",
    "                            individual_article = {}\n",
    "                            individual_article['date'] = article['datePublished'][:10]\n",
    "\n",
    "                            info = info.replace(\"'\",\"\") \n",
    "                            individual_article['text'] = info\n",
    "\n",
    "                            formatted_articles.append(individual_article)\n",
    "\n",
    "                            scraped = scraped + 1;\n",
    "                except:\n",
    "                    continue\n",
    "    #         print(json.dumps(self.formatted_articles, indent=4, sort_keys=True))\n",
    "            if iterationsLeft <= 0:\n",
    "                print(\"Finished Scraping\")\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "                print(\"Scraped Page: \", pageNum, ' Items: ', scraped)\n",
    "                pageNum += 1\n",
    "            \n",
    "        self.formatted_articles = formatted_articles\n",
    "    \n",
    "    \n",
    "    # Sentiment analysis using google cloud\n",
    "    def __sentiment_analysis(self):\n",
    "        client = self.client\n",
    "        tone_analyzed_articles = {'date':[],'score':[]}\n",
    "        formatted_articles = self.formatted_articles.copy()\n",
    "\n",
    "        # The text to analyze\n",
    "        for i in range(len(formatted_articles)):\n",
    "            analyzed_article = {}\n",
    "            article = formatted_articles[i]\n",
    "            text = article['text']\n",
    "\n",
    "            document = types.Document(content = text, type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "            # Detects the sentiment of the text\n",
    "            sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "            \n",
    "            # Appends to dataframe\n",
    "            tone_analyzed_articles['date'].append(article['date'])\n",
    "            tone_analyzed_articles['score'].append(sentiment.score)\n",
    "        \n",
    "#         print(json.dumps(tone_analyzed_articles, indent=4, sort_keys=True))        \n",
    "        \n",
    "        return pd.DataFrame(tone_analyzed_articles)\n",
    "        \n",
    "    # Dump the dataframe\n",
    "    def __dump_df(self, df):\n",
    "        df.to_csv(r'data/articles/' + self.original + '.csv')\n",
    "        \n",
    "        \n",
    "    # Create the dataframe and save it\n",
    "    def df_create(self):\n",
    "        self.__retrieve_articles()\n",
    "        df = self.__sentiment_analysis()\n",
    "        self.__dump_df(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_api_key = keys['news']\n",
    "json_google_cloud = keys['json_google_cloud']\n",
    "xecd_client = keys['xe']\n",
    "rapid_api_host = keys['rapid_api_host']\n",
    "rapid_api_key = keys['rapid_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the original currency: \n",
      "Enter the new currency: \n"
     ]
    }
   ],
   "source": [
    "print(\"Enter the original currency: \")\n",
    "# cur_original = input()\n",
    "print(\"Enter the new currency: \")\n",
    "# cur_return = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_original = 'USD'\n",
    "cur_return = 'CAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_df = Create_DF(cur_original, cur_return, news_api_key, json_google_cloud, xecd_client,rapid_api_host,rapid_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Start Scraping===============\n",
      "Scraped Page:  1  Items:  9\n",
      "Scraped Page:  2  Items:  15\n",
      "Scraped Page:  3  Items:  14\n",
      "Scraped Page:  4  Items:  19\n",
      "Scraped Page:  5  Items:  15\n",
      "Scraped Page:  6  Items:  10\n",
      "Scraped Page:  7  Items:  12\n",
      "Scraped Page:  8  Items:  13\n",
      "Scraped Page:  9  Items:  4\n",
      "Scraped Page:  10  Items:  9\n",
      "Scraped Page:  11  Items:  20\n",
      "Scraped Page:  12  Items:  19\n",
      "Scraped Page:  13  Items:  17\n",
      "Scraped Page:  14  Items:  17\n",
      "Scraped Page:  15  Items:  22\n",
      "Scraped Page:  16  Items:  19\n",
      "Scraped Page:  17  Items:  16\n",
      "Scraped Page:  18  Items:  17\n",
      "Scraped Page:  19  Items:  14\n",
      "Scraped Page:  20  Items:  13\n",
      "Scraped Page:  21  Items:  0\n",
      "Scraped Page:  22  Items:  0\n",
      "Scraped Page:  23  Items:  0\n",
      "Scraped Page:  24  Items:  0\n",
      "Scraped Page:  25  Items:  0\n",
      "Scraped Page:  26  Items:  0\n",
      "Scraped Page:  27  Items:  0\n",
      "Scraped Page:  28  Items:  0\n",
      "Finished Scraping\n"
     ]
    }
   ],
   "source": [
    "df = created_df.df_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
